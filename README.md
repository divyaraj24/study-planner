# ğŸ“š Studbud: Personalized Study Planner

**Studbud** is an AI-powered web application built using **Streamlit**, which helps students create **customized study plans** based on their individual needs and goals. The app utilizes a **fine-tuned BERT model** for text classification and **Google Gemini** for intelligent information extraction and plan generation.

**Link**: Use [this link](https://study-planner-studbud.streamlit.app/) to use the app deployed on streamlit.

---

## ğŸš€ Features

### 1. **Intelligent Goal Detection**

- Users input a natural-language study intent (e.g., "I want to study DSA for my upcoming exam").
- The system classifies the goal as **Exam**, **Project**, or **Mastery** using a BERT model.

### 2. **Information Extraction with Gemini**

- Extracts structured information from the user input such as:
  - Goal
  - Subject
  - Available study time
  - Current academic level
  - Learning preferences
  - Difficulty level

### 3. **Editable Study Form**

- Pre-filled form fields allow users to review and modify their information.
- Detects and warns about missing or unclear data.

### 4. **Dynamic Study Plan Generator**

- Generates a tailored plan based on:
  - Study scenario (Exam, Project, Mastery)
  - Time availability and difficulty
  - Preferred learning styles
- Includes:
  - Timeline breakdown
  - Daily workload
  - Strategy recommendations

---

## ğŸ’¡ Technologies Used

- **Python**
- **Streamlit** (UI)
- **HuggingFace Transformers** (BERT-based classification)
- **Gemini 2.5 flash** (Information extraction + plan enhancement)
- **Dotenv** (Environment management)

---

## ğŸŒŸ Use Case

This app is ideal for:

- Students preparing for upcoming **exams**
- Learners managing **long-term mastery**
- Anyone organizing a **project-based learning** journey

---

### âš ï¸ BERT Model Disclaimer

This project requires a fine-tuned bert mode uploaded to hugging face, which is accessed using psuedopadel24/bert-study-planner

### ğŸ“¥ Download Instructions

Please use this link below to download the model:

ğŸ”— [Download `bert_study_model.zip`](https://drive.google.com/file/d/1id0M2myASpL34piavcAoKJamqN9_d6B9/view?usp=sharing)

## ğŸ“ Project Structure:

```
study-planner/
â”œâ”€â”€ data/                           # All CSV or raw input/output data
â”‚   â”œâ”€â”€ balanced_study_planner_dataset.csv
â”‚   â”œâ”€â”€ test_3way.csv
â”‚   â””â”€â”€ test_results_3way.csv
â”œâ”€â”€ README.md                       # Project overview and usage
â”œâ”€â”€ requirements.txt                # Dependencies for setup
â”œâ”€â”€ .gitignore                      # Ignored files
â”œâ”€â”€ .env                            # Add environment variables like API keys
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ bert_finetune.py
â””â”€â”€ StudyPlanner/                   # Source code package
    â”œâ”€â”€ studbud.py                  # Streamlit app
    â””â”€â”€ test_model.py
```

# ğŸ§  Fine-Tuned BERT Model for Study Task Classification

## ğŸ” Overview

This project involves fine-tuning a pre-trained **BERT (Bidirectional Encoder Representations from Transformers)** model for a **3-way text classification task** tailored to educational content. The goal of the model is to classify text-based user inputs into one of the following categories:

- **Exam** ğŸ“ â€” Texts related to exam preparation or exam-related study.
- **Project** ğŸ’» â€” Content indicating project-based learning or assignments.
- **Mastery** ğŸ“š â€” Statements reflecting a desire for deep understanding or skill acquisition.

### âœ… Performance Highlights

After fine-tuning the BERT model on a balanced dataset of labeled study planning texts, the model achieved:

- **Overall Accuracy:** `95.51%`
- **Macro F1-Score:** `0.96`

### ğŸ“Š Classification Report

| Label       | Precision | Recall | F1-Score | Support |
| ----------- | --------- | ------ | -------- | ------- |
| **Exam**    | 0.93      | 0.93   | 0.93     | 30      |
| **Mastery** | 0.93      | 0.93   | 0.93     | 30      |
| **Project** | 1.00      | 1.00   | 1.00     | 29      |
| **Overall** |           |        | **0.96** | 89      |

### âš™ï¸ Model Architecture

- **Base Model:** `bert-base-uncased` from Hugging Face Transformers
- **Task Type:** Multi-class sequence classification (3 classes)
- **Token Limit:** Inputs truncated or padded to 128 tokens
- **Epochs:** Trained for 8 epochs
- **Training Batch Size:** 16
- **Evaluation Batch Size:** 8
- **Learning Rate:** `2e-5` with weight decay of `0.01`

### ğŸ§ª Dataset

- Total Examples: `~89` (balanced across the 3 classes)
- Data Split: `80% training`, `20% validation`
- Labels encoded as:
  - `0` â†’ Exam
  - `1` â†’ Project
  - `2` â†’ Mastery

### ğŸ’¾ Model Output

- Trained model and tokenizer saved to `bert_study_model/`
- Exported as a downloadable ZIP file for deployment or reuse

### ğŸ¯ Next Goal

- As the BERT model was trained only on one line inputs the next step is to make a fine tuned model trained on a dataset with multiple line values.

---

## âš ï¸ Disclaimer on Original Guided Project

The original guided project (`StudBud: Study Planner using BERT`) does not generate study plans using BERT. Instead, it passes a prompt to the base BERT model without fine-tuning, which simply picks a number (a class label). The study plan is then hardcoded based on that number â€” not generated by the model itself.

This project uses a **fine-tuned BERT model** to take raw user input and classify it into the correct scenario instead.
