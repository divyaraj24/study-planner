# Studbud: Study Planner using BERT

## âš ï¸ Disclaimer on Original Guided Project

The original guided project (`StudBud: Study Planner using BERT`) does not generate study plans using BERT. Instead, it passes a prompt to the base BERT model without fine-tuning, which simply picks a number (a class label). The study plan is then hardcoded based on that number â€” not generated by the model itself.

This project uses a **fine-tuned BERT model** to take raw user input and classify it into the correct scenario instead.

After which some information is extracted from raw user input in another form to generate a study plan.

## âš ï¸ BERT Model Disclaimer

This project requires a fine-tuned BERT model directory named `bert_study_model` which is **not included** in the GitHub repository due to file size limitations (GitHub restricts files larger than 100MB).

### ðŸ“¥ Download Instructions

Please manually download the model from the link below:

ðŸ”— [Download `bert_study_model.zip`](https://drive.google.com/file/d/1D9jOUnCGUM0TsyYa61uHQqF4gfyOa9oN/view?usp=sharing)  

### ðŸ“¦ Setup Instructions

1. Download the ZIP file from the link above.
2. Extract it into the root of your project directory so the structure looks like what is given below.

## ðŸ“ Project Structure:

```
study-planner/
â”œâ”€â”€ bert_study_model/               # Fine-tuned BERT model files
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ vocab.txt
â”œâ”€â”€ data/                           # All CSV or raw input/output data
â”‚   â”œâ”€â”€ test_3way.csv
|   â”œâ”€â”€ bert_finetune_train.csv
â”‚   â””â”€â”€ test_results_3way.csv
â”œâ”€â”€ StudyPlanner/                   # Source code package
â”‚   â”œâ”€â”€ studbud.py                  # Streamlit app
â”‚   â””â”€â”€ test_model.py               # Model evaluation script
â”œâ”€â”€ scripts/                        # For training, preprocessing, etc.
â”‚   â””â”€â”€ bert_finetune.ipynb         # Your Colab notebook or Python script
â”œâ”€â”€ README.md                       # Project overview and usage
â”œâ”€â”€ .gitignore                      # Ignored files
â””â”€â”€ requirements.txt                # Dependencies for setup
```

# ðŸ“š Fine-Tuning BERT Model for 3-Way Classification:

## ðŸ§  Objective

This project fine-tunes a BERT model to classify study-related text inputs into three scenarios:

- **Exam**: e.g., _"How do I prepare for the exam next week?"_
- **Project**: e.g., _"I need to finish this coding assignment by tomorrow."_
- **Mastery**: e.g., _"I want to understand this topic deeply."_

A fourth class, **Unidentified**, was used during training for ambiguous inputs but ignored during final evaluation to focus on core classifications.

---

## ðŸ—ï¸ Fine-Tuning Summary

The BERT model was fine-tuned using the HuggingFace `transformers` library in **Google Colab** with the following setup:

- **Model**: `bert-base-uncased`
- **Training Epochs**: 4
- **Batch Size**: 8
- **Tokenizer**: `BertTokenizer` (with truncation and padding to max length 128)
- **Optimizer & Scheduler**: Default used via `Trainer`
- **Evaluation Strategy**: Per epoch
- **Dataset**: Custom-labeled CSV with 800 examples across 4 categories (`Exam`, `Project`, `Mastery`, `Unidentified`)
- **Split**: 80/20 stratified train/validation split

After training, the model and tokenizer were saved and exported for local inference.

---

## âœ… Model Testing (3-Way Evaluation)

Using a curated test set of **90 examples** (30 each from `Exam`, `Project`, and `Mastery`), the model was evaluated with the following results:

### ðŸ” Classification Report:

```
              precision    recall  f1-score   support

        Exam       0.96      0.90      0.93        30
     Mastery       0.93      0.93      0.93        30
     Project       0.87      0.93      0.90        29

    accuracy                           0.92        89
   macro avg       0.92      0.92      0.92        89
weighted avg       0.92      0.92      0.92        89
```

ðŸŽ¯ **Overall Accuracy**: `92.13%`

---

## ðŸ“¦ Inference Details

- The model was used locally with PyTorch and HuggingFace Transformers.
- `torch.softmax()` was applied to obtain class probabilities.
- Only the first 3 classes (`Exam`, `Project`, `Mastery`) were considered during evaluation.
- Predictions were paired with confidence scores for each input.

---

## ðŸ–¥ï¸ Streamlit GUI

- Needs to be added

---

## ðŸ“Š Study Plan Generation

Once a scenario (Exam, Project, Mastery) is identified:

- Develop Model to take prompts to generate personalized study plans based on:
  - Goal
  - Subject
  - Study time available -> hours, days..
  - Current Grade
  - Learning Style

BERT model is mainly used for classification and Question/Answering with context so the prompt will be used to extract the above details from the raw user input text.

Once the required information is extracted it can be used to make an apropriate study plan.
