# Studbud: Study Planner using BERT

## ‚ö†Ô∏è BERT Model Disclaimer

This project requires a fine-tuned BERT model directory named `bert_study_model` which is **not included** in the GitHub repository due to file size limitations (GitHub restricts files larger than 100MB).

### üì• Download Instructions

Please manually download the model from the link below:

üîó [Download `bert_study_model.zip`](https://drive.google.com/file/d/1D9jOUnCGUM0TsyYa61uHQqF4gfyOa9oN/view?usp=sharing)  

### üì¶ Setup Instructions

1. Download the ZIP file from the link above.
2. Extract it into the root of your project directory so the structure looks like what is given below.

## üìÅ Project Structure:

```
study-planner/
‚îú‚îÄ‚îÄ bert_study_model/               # Fine-tuned BERT model files
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îú‚îÄ‚îÄ special_tokens_map.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îî‚îÄ‚îÄ vocab.txt
‚îú‚îÄ‚îÄ data/                           # All CSV or raw input/output data
‚îÇ   ‚îú‚îÄ‚îÄ test_3way.csv
|   ‚îú‚îÄ‚îÄ bert_finetune_train.csv
‚îÇ   ‚îî‚îÄ‚îÄ test_results_3way.csv
‚îú‚îÄ‚îÄ StudyPlanner/                   # Source code package
‚îÇ   ‚îú‚îÄ‚îÄ studbud.py                  # Streamlit app
‚îÇ   ‚îî‚îÄ‚îÄ test_model.py               # Model evaluation script
‚îú‚îÄ‚îÄ scripts/                        # For training, preprocessing, etc.
‚îÇ   ‚îî‚îÄ‚îÄ bert_finetune.ipynb         # Your Colab notebook or Python script
‚îú‚îÄ‚îÄ README.md                       # Project overview and usage
‚îú‚îÄ‚îÄ .gitignore                      # Ignored files
‚îî‚îÄ‚îÄ requirements.txt                # Dependencies for setup
```

# üß† Fine-Tuned BERT Model for Study Task Classification

## üîç Overview

This project involves fine-tuning a pre-trained **BERT (Bidirectional Encoder Representations from Transformers)** model for a **3-way text classification task** tailored to educational content. The goal of the model is to classify text-based user inputs into one of the following categories:

- **Exam** üìù ‚Äî Texts related to exam preparation or exam-related study.
- **Project** üíª ‚Äî Content indicating project-based learning or assignments.
- **Mastery** üìö ‚Äî Statements reflecting a desire for deep understanding or skill acquisition.

### ‚úÖ Performance Highlights

After fine-tuning the BERT model on a balanced dataset of labeled study planning texts, the model achieved:

- **Overall Accuracy:** `95.51%`
- **Macro F1-Score:** `0.96`

### üìä Classification Report

| Label    | Precision | Recall | F1-Score | Support |
|----------|-----------|--------|----------|---------|
| **Exam**     | 0.93      | 0.93   | 0.93     | 30      |
| **Mastery**  | 0.93      | 0.93   | 0.93     | 30      |
| **Project**  | 1.00      | 1.00   | 1.00     | 29      |
| **Overall**  |           |        | **0.96** | 89      |

### ‚öôÔ∏è Model Architecture

- **Base Model:** `bert-base-uncased` from Hugging Face Transformers
- **Task Type:** Multi-class sequence classification (3 classes)
- **Token Limit:** Inputs truncated or padded to 128 tokens
- **Epochs:** Trained for 8 epochs
- **Training Batch Size:** 16
- **Evaluation Batch Size:** 8
- **Learning Rate:** `2e-5` with weight decay of `0.01`

### üß™ Dataset

- Total Examples: `~89` (balanced across the 3 classes)
- Data Split: `80% training`, `20% validation`
- Labels encoded as:
  - `0` ‚Üí Exam  
  - `1` ‚Üí Project  
  - `2` ‚Üí Mastery  

### üíæ Model Output

- Trained model and tokenizer saved to `bert_study_model/`
- Exported as a downloadable ZIP file for deployment or reuse

---

## üìä Study Plan Generation

Once a scenario (Exam, Project, Mastery) is identified:

- Develop Model to take prompts to generate personalized study plans based on:
  - Goal
  - Subject
  - Study time available -> hours, days..
  - Current Grade
  - Learning Style

BERT model is mainly used for classification and Question/Answering with context so the prompt will be used to extract the above details from the raw user input text.

Once the required information is extracted it can be used to make an apropriate study plan.

## üñ•Ô∏è Streamlit GUI

- Added GUI (Need to update README)

---

## ‚ö†Ô∏è Disclaimer on Original Guided Project

The original guided project (`StudBud: Study Planner using BERT`) does not generate study plans using BERT. Instead, it passes a prompt to the base BERT model without fine-tuning, which simply picks a number (a class label). The study plan is then hardcoded based on that number ‚Äî not generated by the model itself.

This project uses a **fine-tuned BERT model** to take raw user input and classify it into the correct scenario instead.

After which some information is extracted from raw user input in another form to generate a study plan.